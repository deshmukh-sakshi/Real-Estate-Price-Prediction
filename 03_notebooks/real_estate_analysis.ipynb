{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Estate Price Prediction - Complete Analysis\n",
    "\n",
    "This notebook provides a comprehensive workflow for:\n",
    "1. Data Ingestion - Loading all 6 CSV files\n",
    "2. Data Cleaning - Fixing city typos and removing duplicates\n",
    "3. Data Integration - Merging datasets\n",
    "4. Exploratory Data Analysis\n",
    "5. Feature Engineering\n",
    "6. Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion\n",
    "\n",
    "Load all 6 CSV files from the `01_data_raw/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "data_path = '../01_data_raw/'\n",
    "\n",
    "# List of files to load\n",
    "files = {\n",
    "    'customers': 'customers.csv',\n",
    "    'brokers': 'brokers.csv',\n",
    "    'properties': 'properties.csv',\n",
    "    'deals': 'deals.csv',\n",
    "    'transactions': 'transactions.csv',\n",
    "    'property_features': 'property_features.csv'\n",
    "}\n",
    "\n",
    "# Dictionary to store dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Load each CSV file\n",
    "for key, filename in files.items():\n",
    "    try:\n",
    "        df = pd.read_csv(data_path + filename)\n",
    "        dataframes[key] = df\n",
    "        print(f\"✓ Loaded {filename}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ File not found: {filename}\")\n",
    "        print(f\"  Please add {filename} to the {data_path} folder\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading {filename}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal datasets loaded: {len(dataframes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Exploration\n",
    "\n",
    "Quick overview of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display info for each dataset\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset: {name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(df.head(3))\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    print(f\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "### 2.1 Fix City Name Typos\n",
    "\n",
    "Standardize city names across datasets to fix common typos and inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define city name mappings (common typos and variations)\n",
    "city_mappings = {\n",
    "    'new york': 'New York',\n",
    "    'newyork': 'New York',\n",
    "    'ny': 'New York',\n",
    "    'nyc': 'New York',\n",
    "    'los angeles': 'Los Angeles',\n",
    "    'la': 'Los Angeles',\n",
    "    'losangeles': 'Los Angeles',\n",
    "    'san francisco': 'San Francisco',\n",
    "    'sanfrancisco': 'San Francisco',\n",
    "    'sf': 'San Francisco',\n",
    "    'chicago': 'Chicago',\n",
    "    'chicgo': 'Chicago',\n",
    "    'houston': 'Houston',\n",
    "    'houston ': 'Houston',\n",
    "    'miami': 'Miami',\n",
    "    'mimai': 'Miami'\n",
    "}\n",
    "\n",
    "def clean_city_names(df, city_column='city'):\n",
    "    \"\"\"\n",
    "    Clean and standardize city names in a dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - city_column: name of the column containing city names\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with cleaned city names\n",
    "    \"\"\"\n",
    "    if city_column not in df.columns:\n",
    "        print(f\"Warning: Column '{city_column}' not found in dataframe\")\n",
    "        return df\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert to lowercase for matching\n",
    "    df[city_column] = df[city_column].astype(str).str.strip()\n",
    "    \n",
    "    # Count before cleaning\n",
    "    unique_before = df[city_column].nunique()\n",
    "    \n",
    "    # Replace using mapping (case-insensitive)\n",
    "    df[city_column] = df[city_column].apply(\n",
    "        lambda x: city_mappings.get(x.lower(), x.title())\n",
    "    )\n",
    "    \n",
    "    # Count after cleaning\n",
    "    unique_after = df[city_column].nunique()\n",
    "    \n",
    "    print(f\"City names - Before: {unique_before} unique | After: {unique_after} unique\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply city name cleaning to relevant datasets\n",
    "datasets_with_cities = ['customers', 'properties', 'brokers']\n",
    "\n",
    "for dataset_name in datasets_with_cities:\n",
    "    if dataset_name in dataframes:\n",
    "        print(f\"\\nCleaning {dataset_name}...\")\n",
    "        dataframes[dataset_name] = clean_city_names(dataframes[dataset_name])\n",
    "        print(f\"Unique cities in {dataset_name}: {dataframes[dataset_name]['city'].unique() if 'city' in dataframes[dataset_name].columns else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Remove Duplicate Records\n",
    "\n",
    "Identify and remove duplicate entries from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df, subset=None, keep='first'):\n",
    "    \"\"\"\n",
    "    Remove duplicate rows from a dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - subset: column(s) to consider for identifying duplicates\n",
    "    - keep: which duplicate to keep ('first', 'last', or False to remove all)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame without duplicates\n",
    "    \"\"\"\n",
    "    rows_before = len(df)\n",
    "    df_cleaned = df.drop_duplicates(subset=subset, keep=keep)\n",
    "    rows_after = len(df_cleaned)\n",
    "    duplicates_removed = rows_before - rows_after\n",
    "    \n",
    "    print(f\"Rows before: {rows_before} | Rows after: {rows_after} | Duplicates removed: {duplicates_removed}\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Remove duplicates from each dataset\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\nCleaning {name}...\")\n",
    "    dataframes[name] = remove_duplicates(df)\n",
    "\n",
    "print(\"\\nDuplicate removal complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Handle Missing Values\n",
    "\n",
    "Analyze and handle missing values in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in each dataset\n",
    "print(\"Missing Values Summary:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    \n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\n{name.upper()}:\")\n",
    "        missing_df = pd.DataFrame({\n",
    "            'Missing Count': missing[missing > 0],\n",
    "            'Percentage': missing_pct[missing > 0]\n",
    "        })\n",
    "        print(missing_df)\n",
    "    else:\n",
    "        print(f\"\\n{name.upper()}: No missing values\")\n",
    "\n",
    "# TODO: Implement specific strategies for handling missing values\n",
    "# - Numerical: mean/median imputation or forward fill\n",
    "# - Categorical: mode imputation or 'Unknown' category\n",
    "# - Consider domain knowledge when choosing strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Type Conversions\n",
    "\n",
    "Ensure correct data types for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Convert date columns to datetime\n",
    "date_columns = ['date', 'transaction_date', 'listing_date', 'sale_date', 'created_at', 'updated_at']\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    for col in df.columns:\n",
    "        if any(date_col in col.lower() for date_col in date_columns):\n",
    "            try:\n",
    "                dataframes[name][col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                print(f\"Converted {name}.{col} to datetime\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not convert {name}.{col} to datetime: {e}\")\n",
    "\n",
    "# TODO: Add more data type conversions as needed\n",
    "# - Convert price/amount columns to float\n",
    "# - Convert ID columns to string\n",
    "# - Convert categorical columns to category dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Integration\n",
    "\n",
    "Merge datasets to create a unified dataframe for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example integration strategy:\n",
    "# 1. Start with properties as the base\n",
    "# 2. Join with property_features\n",
    "# 3. Join with deals\n",
    "# 4. Join with transactions\n",
    "# 5. Join with brokers\n",
    "# 6. Join with customers\n",
    "\n",
    "# Note: Actual merge keys depend on your data structure\n",
    "# Common keys: property_id, customer_id, broker_id, deal_id, transaction_id\n",
    "\n",
    "def integrate_datasets(dataframes):\n",
    "    \"\"\"\n",
    "    Integrate multiple datasets into a single dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframes: dictionary of dataframes\n",
    "    \n",
    "    Returns:\n",
    "    - Integrated dataframe\n",
    "    \"\"\"\n",
    "    print(\"Starting data integration...\\n\")\n",
    "    \n",
    "    # Check if required datasets exist\n",
    "    required = ['properties', 'deals']\n",
    "    for req in required:\n",
    "        if req not in dataframes:\n",
    "            print(f\"Warning: Required dataset '{req}' not found\")\n",
    "            return None\n",
    "    \n",
    "    # Start with properties as base\n",
    "    integrated_df = dataframes['properties'].copy()\n",
    "    print(f\"Base dataset (properties): {integrated_df.shape}\")\n",
    "    \n",
    "    # Merge with property_features (if available)\n",
    "    if 'property_features' in dataframes:\n",
    "        # Assuming merge on 'property_id'\n",
    "        print(\"\\nMerging with property_features...\")\n",
    "        # integrated_df = integrated_df.merge(dataframes['property_features'], on='property_id', how='left')\n",
    "        # print(f\"After property_features merge: {integrated_df.shape}\")\n",
    "    \n",
    "    # Merge with deals\n",
    "    if 'deals' in dataframes:\n",
    "        print(\"\\nMerging with deals...\")\n",
    "        # integrated_df = integrated_df.merge(dataframes['deals'], on='property_id', how='left')\n",
    "        # print(f\"After deals merge: {integrated_df.shape}\")\n",
    "    \n",
    "    # Merge with transactions\n",
    "    if 'transactions' in dataframes:\n",
    "        print(\"\\nMerging with transactions...\")\n",
    "        # integrated_df = integrated_df.merge(dataframes['transactions'], on='deal_id', how='left')\n",
    "        # print(f\"After transactions merge: {integrated_df.shape}\")\n",
    "    \n",
    "    # Merge with brokers\n",
    "    if 'brokers' in dataframes:\n",
    "        print(\"\\nMerging with brokers...\")\n",
    "        # integrated_df = integrated_df.merge(dataframes['brokers'], on='broker_id', how='left')\n",
    "        # print(f\"After brokers merge: {integrated_df.shape}\")\n",
    "    \n",
    "    # Merge with customers\n",
    "    if 'customers' in dataframes:\n",
    "        print(\"\\nMerging with customers...\")\n",
    "        # integrated_df = integrated_df.merge(dataframes['customers'], on='customer_id', how='left')\n",
    "        # print(f\"After customers merge: {integrated_df.shape}\")\n",
    "    \n",
    "    print(f\"\\nFinal integrated dataset shape: {integrated_df.shape}\")\n",
    "    return integrated_df\n",
    "\n",
    "# Create integrated dataset\n",
    "# Uncomment when data is available\n",
    "# df_integrated = integrate_datasets(dataframes)\n",
    "\n",
    "print(\"\\nNote: Uncomment the merge operations above once you verify the column names in your datasets\")\n",
    "print(\"Common merge keys: property_id, customer_id, broker_id, deal_id, transaction_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cleaned and Integrated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned individual datasets\n",
    "output_path = '../02_data_processed/'\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    filename = f\"{output_path}{name}_cleaned.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "# Save integrated dataset\n",
    "# Uncomment when integration is complete\n",
    "# if 'df_integrated' in locals():\n",
    "#     df_integrated.to_csv(f\"{output_path}integrated_data.csv\", index=False)\n",
    "#     print(f\"Saved {output_path}integrated_data.csv\")\n",
    "\n",
    "print(\"\\nAll cleaned datasets saved to 02_data_processed/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 4.1 Price Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'price' column exists in properties or integrated dataset\n",
    "# Uncomment and modify based on your data\n",
    "\n",
    "# # Price distribution\n",
    "# plt.figure(figsize=(14, 5))\n",
    "# \n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.hist(df_integrated['price'], bins=50, edgecolor='black')\n",
    "# plt.xlabel('Price')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Price Distribution')\n",
    "# \n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.boxplot(df_integrated['price'])\n",
    "# plt.ylabel('Price')\n",
    "# plt.title('Price Boxplot')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# \n",
    "# # Summary statistics\n",
    "# print(\"Price Statistics:\")\n",
    "# print(df_integrated['price'].describe())\n",
    "\n",
    "print(\"Uncomment the code above once you have the integrated dataset with a 'price' column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select numerical columns\n",
    "# numerical_cols = df_integrated.select_dtypes(include=[np.number]).columns\n",
    "# \n",
    "# # Calculate correlation matrix\n",
    "# correlation_matrix = df_integrated[numerical_cols].corr()\n",
    "# \n",
    "# # Plot heatmap\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "# plt.title('Correlation Heatmap')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# \n",
    "# # Show correlations with price (target variable)\n",
    "# if 'price' in correlation_matrix.columns:\n",
    "#     price_corr = correlation_matrix['price'].sort_values(ascending=False)\n",
    "#     print(\"\\nCorrelations with Price:\")\n",
    "#     print(price_corr)\n",
    "\n",
    "print(\"Uncomment the code above once you have the integrated dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Categorical Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analyze categorical variables\n",
    "# categorical_cols = df_integrated.select_dtypes(include=['object', 'category']).columns\n",
    "# \n",
    "# for col in categorical_cols[:5]:  # First 5 categorical columns\n",
    "#     print(f\"\\n{col}:\")\n",
    "#     print(df_integrated[col].value_counts().head(10))\n",
    "#     \n",
    "#     # Plot if reasonable number of categories\n",
    "#     if df_integrated[col].nunique() <= 10:\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         df_integrated[col].value_counts().plot(kind='bar')\n",
    "#         plt.title(f'Distribution of {col}')\n",
    "#         plt.xlabel(col)\n",
    "#         plt.ylabel('Count')\n",
    "#         plt.xticks(rotation=45)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "print(\"Uncomment the code above once you have the integrated dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Create new features that might improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_features(df):\n",
    "#     \"\"\"\n",
    "#     Create new features from existing columns\n",
    "#     \"\"\"\n",
    "#     df = df.copy()\n",
    "#     \n",
    "#     # Example features:\n",
    "#     \n",
    "#     # 1. Price per square foot\n",
    "#     if 'price' in df.columns and 'square_feet' in df.columns:\n",
    "#         df['price_per_sqft'] = df['price'] / df['square_feet']\n",
    "#     \n",
    "#     # 2. Property age\n",
    "#     if 'year_built' in df.columns:\n",
    "#         df['property_age'] = datetime.now().year - df['year_built']\n",
    "#     \n",
    "#     # 3. Total rooms\n",
    "#     if 'bedrooms' in df.columns and 'bathrooms' in df.columns:\n",
    "#         df['total_rooms'] = df['bedrooms'] + df['bathrooms']\n",
    "#     \n",
    "#     # 4. Has garage (binary feature)\n",
    "#     if 'garage' in df.columns:\n",
    "#         df['has_garage'] = (df['garage'] > 0).astype(int)\n",
    "#     \n",
    "#     # 5. Days on market\n",
    "#     if 'listing_date' in df.columns and 'sale_date' in df.columns:\n",
    "#         df['days_on_market'] = (df['sale_date'] - df['listing_date']).dt.days\n",
    "#     \n",
    "#     return df\n",
    "# \n",
    "# # Apply feature engineering\n",
    "# df_engineered = create_features(df_integrated)\n",
    "# print(f\"Features after engineering: {df_engineered.shape[1]}\")\n",
    "# print(f\"New columns: {set(df_engineered.columns) - set(df_integrated.columns)}\")\n",
    "\n",
    "print(\"Uncomment the code above once you have the integrated dataset\")\n",
    "print(\"Customize the feature engineering based on your specific columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling\n",
    "\n",
    "### 6.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# # Prepare data for modeling\n",
    "# def prepare_data(df, target_column='price'):\n",
    "#     \"\"\"\n",
    "#     Prepare data for machine learning\n",
    "#     \"\"\"\n",
    "#     # Separate features and target\n",
    "#     X = df.drop(columns=[target_column])\n",
    "#     y = df[target_column]\n",
    "#     \n",
    "#     # Handle categorical variables\n",
    "#     categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "#     \n",
    "#     for col in categorical_cols:\n",
    "#         le = LabelEncoder()\n",
    "#         X[col] = le.fit_transform(X[col].astype(str))\n",
    "#     \n",
    "#     # Split data\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X, y, test_size=0.2, random_state=42\n",
    "#     )\n",
    "#     \n",
    "#     # Scale features\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_test_scaled = scaler.transform(X_test)\n",
    "#     \n",
    "#     return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "# \n",
    "# # Prepare the data\n",
    "# X_train, X_test, y_train, y_test, scaler = prepare_data(df_engineered)\n",
    "# \n",
    "# print(f\"Training set size: {X_train.shape}\")\n",
    "# print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "print(\"Uncomment the code above once you have the integrated dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "#     \"\"\"\n",
    "#     Train a model and evaluate its performance\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nTraining {model_name}...\")\n",
    "#     \n",
    "#     # Train\n",
    "#     model.fit(X_train, y_train)\n",
    "#     \n",
    "#     # Predict\n",
    "#     y_pred_train = model.predict(X_train)\n",
    "#     y_pred_test = model.predict(X_test)\n",
    "#     \n",
    "#     # Evaluate\n",
    "#     train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "#     test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "#     train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "#     test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "#     train_r2 = r2_score(y_train, y_pred_train)\n",
    "#     test_r2 = r2_score(y_test, y_pred_test)\n",
    "#     \n",
    "#     print(f\"\\n{model_name} Results:\")\n",
    "#     print(f\"Train RMSE: ${train_rmse:,.2f}\")\n",
    "#     print(f\"Test RMSE: ${test_rmse:,.2f}\")\n",
    "#     print(f\"Train MAE: ${train_mae:,.2f}\")\n",
    "#     print(f\"Test MAE: ${test_mae:,.2f}\")\n",
    "#     print(f\"Train R²: {train_r2:.4f}\")\n",
    "#     print(f\"Test R²: {test_r2:.4f}\")\n",
    "#     \n",
    "#     return model, y_pred_test\n",
    "# \n",
    "# # Dictionary to store models and predictions\n",
    "# models = {}\n",
    "# predictions = {}\n",
    "# \n",
    "# # Linear Regression\n",
    "# lr_model, lr_pred = train_and_evaluate_model(\n",
    "#     LinearRegression(),\n",
    "#     X_train, X_test, y_train, y_test,\n",
    "#     \"Linear Regression\"\n",
    "# )\n",
    "# models['Linear Regression'] = lr_model\n",
    "# predictions['Linear Regression'] = lr_pred\n",
    "# \n",
    "# # Random Forest\n",
    "# rf_model, rf_pred = train_and_evaluate_model(\n",
    "#     RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "#     X_train, X_test, y_train, y_test,\n",
    "#     \"Random Forest\"\n",
    "# )\n",
    "# models['Random Forest'] = rf_model\n",
    "# predictions['Random Forest'] = rf_pred\n",
    "# \n",
    "# # Gradient Boosting\n",
    "# gb_model, gb_pred = train_and_evaluate_model(\n",
    "#     GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "#     X_train, X_test, y_train, y_test,\n",
    "#     \"Gradient Boosting\"\n",
    "# )\n",
    "# models['Gradient Boosting'] = gb_model\n",
    "# predictions['Gradient Boosting'] = gb_pred\n",
    "\n",
    "print(\"Uncomment the code above once you have prepared your data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compare model predictions\n",
    "# plt.figure(figsize=(15, 5))\n",
    "# \n",
    "# for idx, (name, pred) in enumerate(predictions.items(), 1):\n",
    "#     plt.subplot(1, 3, idx)\n",
    "#     plt.scatter(y_test, pred, alpha=0.5)\n",
    "#     plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "#     plt.xlabel('Actual Price')\n",
    "#     plt.ylabel('Predicted Price')\n",
    "#     plt.title(f'{name}')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "print(\"Uncomment the code above once you have trained your models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Feature Importance (for tree-based models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot feature importance for Random Forest\n",
    "# if 'Random Forest' in models:\n",
    "#     feature_importance = pd.DataFrame({\n",
    "#         'feature': df_engineered.drop(columns=['price']).columns,\n",
    "#         'importance': models['Random Forest'].feature_importances_\n",
    "#     }).sort_values('importance', ascending=False).head(15)\n",
    "#     \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "#     plt.xlabel('Importance')\n",
    "#     plt.title('Top 15 Feature Importances (Random Forest)')\n",
    "#     plt.gca().invert_yaxis()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     \n",
    "#     print(\"\\nTop 10 Most Important Features:\")\n",
    "#     print(feature_importance.head(10))\n",
    "\n",
    "print(\"Uncomment the code above once you have trained your Random Forest model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# # Save the best performing model\n",
    "# best_model_name = 'Random Forest'  # Change based on your results\n",
    "# best_model = models[best_model_name]\n",
    "# \n",
    "# # Save model\n",
    "# joblib.dump(best_model, '../02_data_processed/best_model.pkl')\n",
    "# joblib.dump(scaler, '../02_data_processed/scaler.pkl')\n",
    "# \n",
    "# print(f\"Saved {best_model_name} model to 02_data_processed/\")\n",
    "# print(\"Model can be loaded later using: model = joblib.load('02_data_processed/best_model.pkl')\")\n",
    "\n",
    "print(\"Uncomment the code above once you have trained your models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions and Next Steps\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "Document your key findings here:\n",
    "- Best performing model\n",
    "- Most important features\n",
    "- Model performance metrics\n",
    "- Business insights\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Model Improvement**:\n",
    "   - Hyperparameter tuning\n",
    "   - Try advanced models (XGBoost, LightGBM)\n",
    "   - Ensemble methods\n",
    "\n",
    "2. **Additional Features**:\n",
    "   - External data sources (demographics, economic indicators)\n",
    "   - Time-based features (seasonality)\n",
    "   - Geographic features (proximity to amenities)\n",
    "\n",
    "3. **Deployment**:\n",
    "   - Create prediction API\n",
    "   - Build web dashboard\n",
    "   - Set up automated retraining pipeline\n",
    "\n",
    "4. **Further Analysis**:\n",
    "   - Time series analysis\n",
    "   - Market segmentation\n",
    "   - Price trend forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analysis complete!\")\n",
    "print(\"\\nRemember to:\")\n",
    "print(\"1. Add your CSV files to 01_data_raw/\")\n",
    "print(\"2. Uncomment and customize the code based on your actual data structure\")\n",
    "print(\"3. Adjust merge keys and column names to match your datasets\")\n",
    "print(\"4. Experiment with different models and feature engineering techniques\")\n",
    "print(\"5. Document your findings and insights\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
